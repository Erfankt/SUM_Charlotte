{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-26T22:44:05.768618Z",
     "start_time": "2026-01-26T22:44:05.758857Z"
    }
   },
   "source": [
    "import sys, os, warnings, geopandas as gpd, numpy as np, pandas as pd, folium\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "try:\n",
    "    from shapely import make_valid     # Shapely 2.0+\n",
    "    HAS_MAKE_VALID = True\n",
    "except Exception:\n",
    "    from shapely.validation import make_valid   # Shapely 1.8 fallback\n",
    "    HAS_MAKE_VALID = False\n",
    "from CompatnessClasses import CompactnessEngine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from factor_analyzer.rotator import Rotator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "sys.path.append(\"../../../../Main/Erfan net crime/urban-network-analysis/src\")\n",
    "from street_network_processor import StreetNetworkProcessor\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\erfan\\\\Dropbox (UNC Charlotte)\\\\Researches\\\\Erfan Dissertation\\\\Scripts\\\\py\\\\SUM_Charlotte'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:44:17.899453Z",
     "start_time": "2026-01-26T22:44:17.678993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Importing datasets\n",
    "# ABT = gpd.read_file(\"../../../../../Edited Subdivisions Zurikanen Erfan 4-20=2025/subs_plus_new/sub.gdb\",layer='final_10_9_25')\n",
    "# ABT[\"issue_date\"] = ABT[\"issue_date\"].dt.year.astype(str) # This is required for the folium cartography\n",
    "ABT = gpd.read_file(\"../../../Data/Final_dataset/ABT/ABT.gpkg\", layer=\"subdivisions\")\n",
    "\n",
    "\n",
    "# building_footprint_dataset = gpd.read_file('../../../Data/Final_dataset/ABT/outputs_building_overlap/Final_Buildings_SemanticIntegrated.gpkg', columns = [\"geometry\",\"HTD_SQ_FT\"]).to_crs(ABT.crs)\n",
    "# building_footprint_dataset = gpd.read_file('../../../Data/Final_dataset/ABT/outputs_building_overlap/Final_Buildings_SemanticIntegrated.gpkg').to_crs(ABT.crs)\n",
    "# building_footprint_dataset['geometry'] = building_footprint_dataset.geometry.apply(make_valid)\n",
    "# building_footprint_dataset_county = gpd.read_file(\"../../../Data/Original_dataset/Archive/Buildings_footprint_meck_01_24/Buildings.shp\").to_crs(ABT.crs)\n",
    " \n",
    "# tax_parcel_dataset = gpd.read_file(\"../../../Data/Original_dataset/Archive/ParcelZoningZipcode_meck_2023_08/ParcelZoningZipcode.shp\", columns=[\"geometry\",\"bldgtype\"]).to_crs(ABT.crs)\n",
    "# tax_parcel_cama_dataset = gpd.read_file(\"../../../Data/Original_dataset/original.gdb\", layer = \"Tax_CAMA_6_24\", columns=[\"geometry\",\"bldgtype\"]).to_crs(ABT.crs)\n",
    "\n",
    "# source_edges_path = \"../../../../Main/Erfan net crime/data/Streets_Meckl_10_8_2023/processing_files/edges_version_2.shp\"\n",
    "\n",
    "# waterbody = gpd.read_file(\"../../../Data/Original_dataset/Archive/Ponds/Ponds.shp\").to_crs(ABT.crs)\n",
    "\n",
    "# street_network_df = gpd.read_file(\"../../../Data/Final_dataset/street_network_df.shp\")\n",
    "\n",
    "accessibility_ws = pd.read_csv(\"../../../Data/Final_dataset/ABT/WalkScore.csv\")"
   ],
   "id": "97c3b3cf274263e1",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualzing sample sub\n",
    "\n",
    "sub_id_to_check = 4294  # change to one that exists\n",
    "\n",
    "\n",
    "sub_row = ABT[ABT[\"subd_id\"] == sub_id_to_check].iloc[0]\n",
    "sub_geom = sub_row.geometry\n",
    "\n",
    "# Find intersecting and clipped buildings\n",
    "intersecting_buildings = building_footprint_dataset[building_footprint_dataset.intersects(sub_geom)].copy()\n",
    "intersecting_buildings[\"geometry\"] = intersecting_buildings.geometry.intersection(sub_geom)\n",
    "intersecting_buildings = intersecting_buildings[~intersecting_buildings.geometry.is_empty]\n",
    "\n",
    "# Convert to WGS84 for folium\n",
    "sub_vis = gpd.GeoDataFrame([sub_row], crs=ABT.crs).to_crs(epsg=4326)\n",
    "build_vis = intersecting_buildings.to_crs(epsg=4326)\n",
    "\n",
    "# Initialize map\n",
    "centroid = sub_vis.geometry.centroid.iloc[0]\n",
    "m = folium.Map(location=[centroid.y, centroid.x], zoom_start=16, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Add subdivision boundary\n",
    "folium.GeoJson(\n",
    "    sub_vis,\n",
    "    name=\"Subdivision\",\n",
    "    style_function=lambda f: {\"color\": \"black\", \"weight\": 2, \"fill\": False}\n",
    ").add_to(m)\n",
    "\n",
    "# Add buildings (clipped)\n",
    "folium.GeoJson(\n",
    "    build_vis,\n",
    "    name=\"Buildings (clipped)\",\n",
    "    style_function=lambda f: {\"color\": \"red\", \"weight\": 1, \"fillOpacity\": 0.5},\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ],
   "id": "14c469de4b396100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __1. Measuring the 1st indicator (Cohesion Index Intra-subdivision)__",
   "id": "fa1a2249e30c3996"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**note: I ran the code in HPC environment**",
   "id": "6137158a60548a5f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2026-01-26T05:00:39.723947500Z",
     "start_time": "2026-01-26T05:00:05.376843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "#  STAGE 1: COMPUTE CONTENT-LEVEL COMPACTNESS METRICS\n",
    "# ============================================================\n",
    "content_cols = [\n",
    "    \"AI\",        # Aggregation Index (higher = more compact)\n",
    "    \"PROX\",      # Proximity Index   (higher = more compact)\n",
    "    \"ENN_MN\",    # Nearest-Neighbor Distance (lower = more compact ‚Üí inverted later)\n",
    "    \"ED\",        # Edge Density      (lower = more compact ‚Üí inverted later)\n",
    "    \"SHAPE_MN\",\n",
    "    \"FRAC_MN\"\n",
    "]\n",
    "\n",
    "# Initialize columns in ABT\n",
    "for col in content_cols:\n",
    "    ABT[col] = None\n",
    "\n",
    "# Initialize Compactness Engine\n",
    "compact_engine = CompactnessEngine(\n",
    "    buildings_gdf=building_footprint_dataset.to_crs(ABT.crs)\n",
    ")\n",
    "\n",
    "# Helper function\n",
    "def compute_compactness_for_row(sub_geom):\n",
    "    try:\n",
    "        metrics = compact_engine.compute_for_subdivision(\n",
    "            subdivision=sub_geom\n",
    "        )\n",
    "        if metrics is None:\n",
    "            return None\n",
    "        return {col: metrics.get(col, None) for col in content_cols}\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n",
    "        return None\n",
    "\n",
    "# Compute for all subdivisions\n",
    "tqdm.pandas()\n",
    "\n",
    "ABT[\"compactness_dict\"] = ABT.geometry.progress_apply(\n",
    "    compute_compactness_for_row\n",
    ")\n",
    "\n",
    "for col in content_cols:\n",
    "    ABT[col] = ABT[\"compactness_dict\"].apply(\n",
    "        lambda d: d.get(col) if isinstance(d, dict) else None\n",
    "    )\n",
    "\n",
    "ABT.drop(columns=[\"compactness_dict\"], inplace=True)\n",
    "print(\"‚úÖ Finished Stage 1: Extracted content-level compactness metrics.\")"
   ],
   "id": "cb0a77ea7f5a0b22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__ENTROPY__",
   "id": "71090aa677117ba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T18:49:14.453622Z",
     "start_time": "2025-12-05T18:38:12.688147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Step 2A ‚Äî REORIENTATION (ensure higher = more compact)\n",
    "# ============================================================\n",
    "\n",
    "ABT[\"ENN_inv\"] = 1 / (ABT[\"ENN_MN\"] + 1e-6)   # invert distance\n",
    "ABT[\"ED_inv\"]  = 1 / (ABT[\"ED\"] + 1e-6)       # invert edge density\n",
    "ABT[\"SHAPE_inv\"] = 1 / (ABT[\"SHAPE_MN\"] + 1e-6)  # invert shape mean\n",
    "ABT[\"FRAC_inv\"]  = 1 / (ABT[\"FRAC_MN\"]  + 1e-6)  # invert fractal mean\n",
    "\n",
    "# Define final metrics used in composite index\n",
    "main_metrics = [\"AI\", \"PROX\", \"ENN_inv\", \"ED_inv\"]\n",
    "optional_metrics = [\"SHAPE_inv\", \"FRAC_inv\"]\n",
    "\n",
    "all_metrics = main_metrics + optional_metrics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Step 2B ‚Äî NORMALIZATION (ONE TIME)\n",
    "# ============================================================\n",
    "\n",
    "def min_max_normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min() + 1e-9)\n",
    "\n",
    "for m in all_metrics:\n",
    "    ABT[m + \"_norm\"] = min_max_normalize(ABT[m])\n",
    "\n",
    "print(\"‚úÖ Finished Stage 2: Reoriented & normalized all metrics.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Step 3A ‚Äî ENTROPY WEIGHTS\n",
    "# ============================================================\n",
    "\n",
    "def compute_entropy_weights(df, metric_cols):\n",
    "    X = df[metric_cols].fillna(0).values\n",
    "\n",
    "    # Convert to proportions across subdivisions\n",
    "    P = X / (X.sum(axis=0, keepdims=True) + 1e-12)\n",
    "\n",
    "    # Compute entropy\n",
    "    k = 1 / np.log(len(df))\n",
    "    entropy = -k * np.sum(P * np.log(P + 1e-12), axis=0)\n",
    "\n",
    "    # Compute redundancy\n",
    "    d = 1 - entropy\n",
    "\n",
    "    # Normalize to weights\n",
    "    w = d / d.sum()\n",
    "\n",
    "    return pd.Series(w, index=metric_cols)\n",
    "\n",
    "entropy_weights = compute_entropy_weights(\n",
    "    ABT, \n",
    "    [m + \"_norm\" for m in all_metrics]\n",
    ")\n",
    "\n",
    "print(\"\\nüìå ENTROPY WEIGHTS:\")\n",
    "print(entropy_weights)\n",
    "print(\"Sum of weights:\", entropy_weights.sum())\n",
    "\n",
    "\n",
    "# Final compactness score = weighted sum of normalized metrics\n",
    "\n",
    "ABT[\"COMPACTNESS_SUM\"] = 0\n",
    "\n",
    "for m in all_metrics:\n",
    "    ABT[\"COMPACTNESS_SUM\"] += (\n",
    "        ABT[m + \"_norm\"] * entropy_weights[m + \"_norm\"]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Stage 3 complete: Final COMPACTNESS_SUM index created.\")"
   ],
   "id": "d2bff1698a6f0b99",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8376/8376 [05:23<00:00, 25.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished Stage 1: Extracted content-level compactness metrics.\n",
      "‚úÖ Finished Stage 2: Reoriented & normalized all metrics.\n",
      "\n",
      "üìå ENTROPY WEIGHTS:\n",
      "AI_norm         0.0023\n",
      "PROX_norm       0.1411\n",
      "ENN_inv_norm    0.3642\n",
      "ED_inv_norm     0.3049\n",
      "SHAPE_MN_norm   0.1851\n",
      "FRAC_MN_norm    0.0025\n",
      "dtype: float64\n",
      "Sum of weights: 1.0\n",
      "‚úÖ Stage 3 complete: Final COMPACTNESS_SUM index created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __2. Measuring the 2nd indicator (Inverse Distance to Nearest Human Activity Center Index)__ ",
   "id": "df978c72ab9674f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:19:54.957253Z",
     "start_time": "2026-01-26T06:19:53.134779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"HAC_dist\" not in ABT.columns:\n",
    "    \n",
    "    ABT[\"HAC_dist\"] = None\n",
    "\n",
    "    HAC_dataset = gpd.read_file(\"../../../Data/Original_dataset/original.gdb\",layer='HAC')\n",
    "    \n",
    "    # mapping of HAC locations\n",
    "    HAC_dataset_wgs = HAC_dataset.to_crs(epsg=4326)\n",
    "    \n",
    "    # Get the center of the map based on dataset bounds\n",
    "    bounds = HAC_dataset_wgs.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    center = [(bounds[1] + bounds[3])/2, (bounds[0] + bounds[2])/2]\n",
    "    \n",
    "    # Create a Folium map\n",
    "    m = folium.Map(location=center, zoom_start=12)\n",
    "    \n",
    "    # Add the HAC dataset\n",
    "    folium.GeoJson(HAC_dataset_wgs).add_to(m);m\n",
    "    \n",
    "    nearest_distances = gpd.sjoin_nearest(\n",
    "    ABT,\n",
    "    HAC_dataset[['geometry']],   # only geometry, no extra columns\n",
    "    how='left',\n",
    "    distance_col='HAC_dist'\n",
    "    )\n",
    "\n",
    "    # Add distance in mile as a new column in the original GeoDataFrame\n",
    "    ABT['HAC_dist'] = (nearest_distances['HAC_dist'] / 5280).round(2)\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ HAC Index already exist).\")"
   ],
   "id": "729aa7db743b5526",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __3. measuring the 3rd indicator (Cohesion Index Contextual)__",
   "id": "3c0c97f547cdfcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**note: I ran the code in HPC environment**\n",
   "id": "87032e5d74afe7ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T00:31:07.999841Z",
     "start_time": "2025-12-05T23:27:55.009554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# CONTEXTUAL COMPACTNESS AROUND SUBDIVISIONS (UPDATED & FINAL)\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0. METRIC LISTS (ALL ALREADY DIRECTIONALLY ALIGNED)\n",
    "# ---------------------------------------------------------\n",
    "ctx_content_cols = [\n",
    "    \"AI\",          # Aggregation Index (higher = more compact)\n",
    "    \"PROX\",        # Proximity Index (higher = more compact)\n",
    "    \"ENN_inv\",     # Inverted ENN_MN\n",
    "    \"ED_inv\",      # Inverted Edge Density\n",
    "    \"SHAPE_inv\",   # Inverted Mean Shape Index\n",
    "    \"FRAC_inv\"     # Inverted Mean Fractal Dimension\n",
    "]\n",
    "\n",
    "# Initialize dictionary columns\n",
    "ABT[\"ctx025_dict\"] = None\n",
    "ABT[\"ctx050_dict\"] = None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. COMPACTNESS ENGINE (same as subdivision-level)\n",
    "# ---------------------------------------------------------\n",
    "compact_engine = CompactnessEngine(\n",
    "    buildings_gdf=building_footprint_dataset.to_crs(ABT.crs)\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. HELPER FUNCTION ‚Äî CONTEXTUAL METRICS\n",
    "# ---------------------------------------------------------\n",
    "def compute_contextual_for_row(sub_geom, radius, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute contextual compactness metrics within a circular buffer\n",
    "    centered on the subdivision centroid, excluding buildings inside\n",
    "    the subdivision itself.\n",
    "    \"\"\"\n",
    "\n",
    "    centroid = sub_geom.centroid\n",
    "    buffer_geom = centroid.buffer(radius)\n",
    "\n",
    "    # Spatial index query\n",
    "    idx = list(building_footprint_dataset.sindex.intersection(buffer_geom.bounds))\n",
    "    possible = building_footprint_dataset.iloc[idx]\n",
    "\n",
    "    # Buildings inside buffer but outside the subdivision\n",
    "    bld = possible[\n",
    "        possible.geometry.intersects(buffer_geom)\n",
    "        & ~possible.geometry.within(sub_geom)\n",
    "    ]\n",
    "\n",
    "    if len(bld) == 0:\n",
    "        return None\n",
    "\n",
    "    geoms = bld.geometry.tolist()\n",
    "\n",
    "    # Rasterize buildings inside buffer\n",
    "    raster = compact_engine._rasterize_buildings(\n",
    "        geoms, buffer_geom, resolution\n",
    "    )\n",
    "\n",
    "    # ------------------ METRIC COMPUTATION ------------------\n",
    "\n",
    "    # Aggregation Index\n",
    "    AI = compact_engine.aggregation_index(raster)\n",
    "\n",
    "    # Proximity Index (FRAGSTATS-style)\n",
    "    PROX = compact_engine.proximity_index(\n",
    "        geoms=geoms,\n",
    "        subdivision=buffer_geom,\n",
    "        resolution=resolution,\n",
    "        search_radius=100\n",
    "    )\n",
    "\n",
    "    # Euclidean Nearest Neighbor (patch-based)\n",
    "    ENN_MN = compact_engine.enn_mn_raster(\n",
    "        raster=raster,\n",
    "        subdivision=buffer_geom,\n",
    "        resolution=resolution\n",
    "    )\n",
    "    ENN_inv = 1 / (ENN_MN + 1e-6)\n",
    "\n",
    "    # Edge Density (class-level, m/ha)\n",
    "    ED = compact_engine.edge_density_raster(\n",
    "        raster=raster,\n",
    "        subdivision=buffer_geom,\n",
    "        resolution_feet=resolution\n",
    "    )\n",
    "    ED_inv = 1 / (ED + 1e-6)\n",
    "\n",
    "    # Shape metrics (building geometry)\n",
    "    SHAPE_MN = float(np.mean([\n",
    "        compact_engine.building_shape_index(g) for g in geoms\n",
    "    ]))\n",
    "    FRAC_MN = float(np.nanmean([\n",
    "        compact_engine.building_fractal_dimension(g) for g in geoms\n",
    "    ]))\n",
    "\n",
    "    SHAPE_inv = 1 / (SHAPE_MN + 1e-6)\n",
    "    FRAC_inv  = 1 / (FRAC_MN  + 1e-6)\n",
    "\n",
    "    return {\n",
    "        \"AI\": AI,\n",
    "        \"PROX\": PROX,\n",
    "        \"ENN_inv\": ENN_inv,\n",
    "        \"ED_inv\": ED_inv,\n",
    "        \"SHAPE_inv\": SHAPE_inv,\n",
    "        \"FRAC_inv\": FRAC_inv\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. APPLY TO ALL SUBDIVISIONS\n",
    "# ---------------------------------------------------------\n",
    "tqdm.pandas()\n",
    "\n",
    "# 0.25-mile buffer (1320 ft)\n",
    "ABT[\"ctx025_dict\"] = ABT.geometry.progress_apply(\n",
    "    lambda g: compute_contextual_for_row(g, radius=1320)\n",
    ")\n",
    "\n",
    "# 0.5-mile buffer (2640 ft)\n",
    "ABT[\"ctx050_dict\"] = ABT.geometry.progress_apply(\n",
    "    lambda g: compute_contextual_for_row(g, radius=2640)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Finished Stage 1: Extracted contextual raw metrics.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. EXPAND DICTIONARIES INTO COLUMNS\n",
    "# ---------------------------------------------------------\n",
    "for col in ctx_content_cols:\n",
    "    ABT[f\"{col}_025\"] = ABT[\"ctx025_dict\"].apply(\n",
    "        lambda d: d.get(col) if isinstance(d, dict) else None\n",
    "    )\n",
    "    ABT[f\"{col}_050\"] = ABT[\"ctx050_dict\"].apply(\n",
    "        lambda d: d.get(col) if isinstance(d, dict) else None\n",
    "    )\n",
    "\n",
    "ABT.drop(columns=[\"ctx025_dict\", \"ctx050_dict\"], inplace=True)\n",
    "print(\"üîç Expanded contextual metrics into columns.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. NORMALIZATION (MIN‚ÄìMAX)\n",
    "# ---------------------------------------------------------\n",
    "def min_max_normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min() + 1e-9)\n",
    "\n",
    "# Normalize 0.25-mile metrics\n",
    "for m in ctx_content_cols:\n",
    "    ABT[f\"{m}_025_norm\"] = min_max_normalize(ABT[f\"{m}_025\"])\n",
    "\n",
    "# Normalize 0.5-mile metrics\n",
    "for m in ctx_content_cols:\n",
    "    ABT[f\"{m}_050_norm\"] = min_max_normalize(ABT[f\"{m}_050\"])\n",
    "\n",
    "print(\"‚úÖ Stage 2: Normalized contextual metrics.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. ENTROPY WEIGHTING\n",
    "# ---------------------------------------------------------\n",
    "def compute_entropy_weights(df, metric_cols):\n",
    "    X = df[metric_cols].fillna(0).values\n",
    "\n",
    "    # Proportions\n",
    "    P = X / (X.sum(axis=0, keepdims=True) + 1e-12)\n",
    "\n",
    "    k = 1 / np.log(len(df))\n",
    "    entropy = -k * np.sum(P * np.log(P + 1e-12), axis=0)\n",
    "\n",
    "    redundancy = 1 - entropy\n",
    "    weights = redundancy / redundancy.sum()\n",
    "\n",
    "    return pd.Series(weights, index=metric_cols)\n",
    "\n",
    "\n",
    "cols025 = [m + \"_025_norm\" for m in ctx_content_cols]\n",
    "cols050 = [m + \"_050_norm\" for m in ctx_content_cols]\n",
    "\n",
    "weights025 = compute_entropy_weights(ABT, cols025)\n",
    "weights050 = compute_entropy_weights(ABT, cols050)\n",
    "\n",
    "print(\"\\nüìå ENTROPY WEIGHTS (0.25-mile):\")\n",
    "print(weights025)\n",
    "print(\"\\nüìå ENTROPY WEIGHTS (0.5-mile):\")\n",
    "print(weights050)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. FINAL CONTEXTUAL COMPACTNESS INDICES\n",
    "# ---------------------------------------------------------\n",
    "ABT[\"COMPACTNESS_SUM_ctx_025\"] = 0.0\n",
    "ABT[\"COMPACTNESS_SUM_ctx_050\"] = 0.0\n",
    "\n",
    "for m in ctx_content_cols:\n",
    "    ABT[\"COMPACTNESS_SUM_ctx_025\"] += (\n",
    "        ABT[f\"{m}_025_norm\"] * weights025[f\"{m}_025_norm\"]\n",
    "    )\n",
    "    ABT[\"COMPACTNESS_SUM_ctx_050\"] += (\n",
    "        ABT[f\"{m}_050_norm\"] * weights050[f\"{m}_050_norm\"]\n",
    "    )\n",
    "\n",
    "print(\"üéâ SUCCESS: Created COMPACTNESS_SUM_ctx_025 and COMPACTNESS_SUM_ctx_050.\")"
   ],
   "id": "b1912dd632deab0c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8376/8376 [21:22<00:00,  6.53it/s]  \n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 3148/8376 [37:24<1:02:07,  1.40it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    #Quality control\n",
    "\n",
    "# --- Make sure both GeoDataFrames are in WGS84 (lat/lon) ---\n",
    "subdivision_wgs = ABT.to_crs(4326)\n",
    "buildings_wgs = building_footprint_dataset.to_crs(4326)\n",
    "\n",
    "# --- Pick a sample subdivision by index ---\n",
    "sample_idx = 42  # üëà change this index to test another\n",
    "sub = subdivision_wgs.loc[sample_idx]\n",
    "centroid = sub.geometry.centroid\n",
    "\n",
    "# --- Buffers (recreate in feet, then project to WGS84) ---\n",
    "sub_proj = ABT.loc[[sample_idx]]  # use projected CRS for buffering\n",
    "centroid_proj = sub_proj.geometry.centroid.iloc[0]\n",
    "buffer_025_proj = centroid_proj.buffer(buffer_025_radius)\n",
    "buffer_050_proj = centroid_proj.buffer(buffer_050_radius)\n",
    "\n",
    "# Convert buffers to GeoDataFrames and reproject to WGS84\n",
    "buffer_025 = gpd.GeoSeries([buffer_025_proj], crs=ABT.crs).to_crs(4326).iloc[0]\n",
    "buffer_050 = gpd.GeoSeries([buffer_050_proj], crs=ABT.crs).to_crs(4326).iloc[0]\n",
    "\n",
    "# --- Buildings intersecting buffers (still use projected CRS for accuracy) ---\n",
    "bldgs_025_proj = building_footprint_dataset[\n",
    "    building_footprint_dataset.geometry.intersects(buffer_025_proj) &\n",
    "    ~building_footprint_dataset.geometry.within(sub_proj.geometry.iloc[0])\n",
    "]\n",
    "bldgs_050_proj = building_footprint_dataset[\n",
    "    building_footprint_dataset.geometry.intersects(buffer_050_proj) &\n",
    "    ~building_footprint_dataset.geometry.within(sub_proj.geometry.iloc[0])\n",
    "]\n",
    "\n",
    "# Reproject selected buildings to WGS84 for Folium\n",
    "bldgs_025 = bldgs_025_proj.to_crs(4326)\n",
    "bldgs_050 = bldgs_050_proj.to_crs(4326)\n",
    "\n",
    "# --- Create Folium map centered on the subdivision centroid ---\n",
    "m = folium.Map(location=[centroid.y, centroid.x], zoom_start=16, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Subdivision polygon\n",
    "folium.GeoJson(sub.geometry, name=\"Subdivision\",\n",
    "               style_function=lambda x: {\"color\": \"blue\", \"weight\": 3, \"fill\": False}).add_to(m)\n",
    "\n",
    "# Buffers\n",
    "folium.GeoJson(buffer_025, name=\"0.25 mi Buffer\",\n",
    "               style_function=lambda x: {\"color\": \"orange\", \"weight\": 2, \"fill\": False}).add_to(m)\n",
    "folium.GeoJson(buffer_050, name=\"0.5 mi Buffer\",\n",
    "               style_function=lambda x: {\"color\": \"red\", \"weight\": 2, \"fill\": False}).add_to(m)\n",
    "\n",
    "# Buildings\n",
    "folium.GeoJson(bldgs_025, name=\"Buildings (0.25 mi)\",\n",
    "               style_function=lambda x: {\"color\": \"orange\", \"weight\": 1}).add_to(m)\n",
    "folium.GeoJson(bldgs_050, name=\"Buildings (0.5 mi)\",\n",
    "               style_function=lambda x: {\"color\": \"red\", \"weight\": 1}).add_to(m)\n",
    "\n",
    "# Centroid marker\n",
    "folium.Marker(\n",
    "    location=[centroid.y, centroid.x],\n",
    "    popup=(f\"<b>Subdivision {sample_idx}</b><br>\"\n",
    "           f\"CI (0.25 mi): {ABT.loc[sample_idx, 'ci_025mi']}<br>\"\n",
    "           f\"CI (0.5 mi): {ABT.loc[sample_idx, 'ci_050mi']}\"),\n",
    "    icon=folium.Icon(color=\"blue\", icon=\"home\")\n",
    ").add_to(m)\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ],
   "id": "c1dac12a29a9d80b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __4. Measuring the 4th indicator (BAD)__",
   "id": "51fe391174990f0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'BAD' in ABT.columns:\n",
    "    \n",
    "    ABT['BAD']= None\n",
    "\n",
    "    # --- 2) Fix invalid geometries ---\n",
    "    if HAS_MAKE_VALID:\n",
    "        ABT['geometry'] = ABT.geometry.map(make_valid)\n",
    "        building_footprint_dataset['geometry'] = building_footprint_dataset.geometry.map(make_valid)\n",
    "    else:\n",
    "        ABT['geometry'] = ABT.buffer(0)\n",
    "        building_footprint_dataset['geometry'] = building_footprint_dataset.buffer(0)\n",
    "    \n",
    "    # --- 3) Compute subdivision land area ---\n",
    "    ABT['land_area'] = ABT.geometry.area\n",
    "    \n",
    "    # --- 4) Overlay in chunks to track progress ---\n",
    "    subs = ABT[['subd_id', 'geometry']]\n",
    "    blds = building_footprint_dataset[['geometry']]\n",
    "    \n",
    "    chunk_size = 5000  # adjust depending on your RAM\n",
    "    intersections_list = []\n",
    "    \n",
    "    print(\"Performing overlay in chunks...\")\n",
    "    for i in tqdm(range(0, len(blds), chunk_size), desc=\"Overlay progress\"):\n",
    "        chunk = blds.iloc[i:i+chunk_size]\n",
    "        inter = gpd.overlay(chunk, subs, how='intersection', keep_geom_type=True)\n",
    "        intersections_list.append(inter)\n",
    "    \n",
    "    intersections = pd.concat(intersections_list, ignore_index=True)\n",
    "    \n",
    "    # --- 5) Sum intersected areas per subdivision ---\n",
    "    print(\"Computing built-up area sums...\")\n",
    "    intersections['built_piece_area'] = intersections.geometry.area\n",
    "    built_sum = (intersections\n",
    "                 .groupby('subd_id', as_index=False)['built_piece_area']\n",
    "                 .sum()\n",
    "                 .rename(columns={'built_piece_area': 'built_area'}))\n",
    "    \n",
    "    # --- 6) Merge back and compute BAD ---\n",
    "    print(\"Final merge and ratio computation...\")\n",
    "    ABT = ABT.merge(built_sum, on='subd_id', how='left')\n",
    "    ABT['built_area'] = ABT['built_area'].fillna(0.0)\n",
    "    ABT['BAD'] = (ABT['built_area'] / ABT['land_area']).replace([float('inf')], 0).round(3)\n",
    "else:\n",
    "    print(\"‚úÖ BAD Index already exist).\")"
   ],
   "id": "65e3912f20a1b83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __5. Measuring the 5th indicator (FAR)__",
   "id": "8825fe3d00056e75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:23:07.932664Z",
     "start_time": "2026-01-26T06:22:45.791557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'FAR'  in ABT.columns:\n",
    "    \n",
    "    ABT[\"FAR\"] = None\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 1. CREATE INDEX-BASED UNIQUE BUILDING ID\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Assigning index-based building IDs...\")\n",
    "    \n",
    "    building_footprint_dataset = (\n",
    "        building_footprint_dataset.reset_index()\n",
    "        .rename(columns={\"index\": \"bldg_idx\"})\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 2. RUN SPATIAL OVERLAY / INTERSECTION\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Running spatial overlay/intersection (this may take time)...\")\n",
    "    \n",
    "    intersections = gpd.overlay(\n",
    "        building_footprint_dataset[[\"bldg_idx\", \"geometry\"]],\n",
    "        ABT[[\"subd_id\", \"geometry\"]],\n",
    "        how=\"intersection\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Overlay complete. {len(intersections)} intersecting building pieces.\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 3. COMPUTE AREAS\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Computing intersection and building footprint areas...\")\n",
    "    \n",
    "    intersections[\"INTERSECT_AREA\"] = intersections.geometry.area\n",
    "    building_footprint_dataset[\"BLDG_AREA\"] = building_footprint_dataset.geometry.area\n",
    "    \n",
    "    print(\"Areas computed.\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 4. MERGE BUILDING ATTRIBUTES BACK INTO INTERSECTIONS\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Merging building attributes back into intersections...\")\n",
    "    \n",
    "    intersections = intersections.merge(\n",
    "        building_footprint_dataset[[\"bldg_idx\", \"BLDG_AREA\", \"HTD_SQ_FT\"]],\n",
    "        on=\"bldg_idx\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(\"Attributes merged.\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 5. CALCULATE PROPORTIONAL HEATED SQ FT\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Calculating proportional heated square feet...\")\n",
    "    \n",
    "    intersections[\"AREA_PROPORTION\"] = (\n",
    "        intersections[\"INTERSECT_AREA\"] / intersections[\"BLDG_AREA\"]\n",
    "    ).clip(upper=1)\n",
    "    \n",
    "    intersections[\"HTD_SQ_FT_PROPORTIONAL\"] = (\n",
    "        intersections[\"HTD_SQ_FT\"] * intersections[\"AREA_PROPORTION\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Proportional HTD_SQ_FT computed.\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 6. AGGREGATE BY PARCEL (subd_id)\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Aggregating proportional heated sqft by subd_id...\")\n",
    "    \n",
    "    htd_sum = (\n",
    "        intersections\n",
    "        .groupby(\"subd_id\")[\"HTD_SQ_FT_PROPORTIONAL\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"HTD_SQ_FT_PROPORTIONAL\": \"TOTAL_HTD_SQ_FT\"})\n",
    "    )\n",
    "    \n",
    "    print(\"Aggregation complete.\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 7. COMPUTE PARCEL AREA\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Computing parcel areas...\")\n",
    "    \n",
    "    ABT[\"PARCEL_AREA_SQFT\"] = ABT.geometry.area\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 8. MERGE HEATED AREA BACK INTO ABT\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Merging heated sqft into ABT...\")\n",
    "    \n",
    "    ABT = ABT.merge(htd_sum, on=\"subd_id\", how=\"left\")\n",
    "    ABT[\"TOTAL_HTD_SQ_FT\"] = ABT[\"TOTAL_HTD_SQ_FT\"].fillna(0)\n",
    "    \n",
    "    print(\"Merge complete.\\n\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 9. COMPUTE FAR\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    print(\"Computing FAR...\")\n",
    "    \n",
    "    ABT[\"FAR\"] = ABT[\"TOTAL_HTD_SQ_FT\"] / ABT[\"PARCEL_AREA_SQFT\"]\n",
    "    \n",
    "    print(\"FAR calculation completed.\\n\")"
   ],
   "id": "8e6cbd3c52a49b7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning index-based building IDs...\n",
      "Running spatial overlay/intersection (this may take time)...\n",
      "Overlay complete. 240687 intersecting building pieces.\n",
      "\n",
      "Computing intersection and building footprint areas...\n",
      "Areas computed.\n",
      "\n",
      "Merging building attributes back into intersections...\n",
      "Attributes merged.\n",
      "\n",
      "Calculating proportional heated square feet...\n",
      "Proportional HTD_SQ_FT computed.\n",
      "\n",
      "Aggregating proportional heated sqft by subd_id...\n",
      "Aggregation complete.\n",
      "\n",
      "Computing parcel areas...\n",
      "Merging heated sqft into ABT...\n",
      "Merge complete.\n",
      "\n",
      "Computing FAR...\n",
      "FAR calculation completed.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __6. Measuring the 6th indicator (SHD)__\n",
   "id": "368b20fa95df14c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:29:43.933450Z",
     "start_time": "2026-01-26T06:26:41.712141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'SHD' in ABT.columns:\n",
    "\n",
    "    ABT['SHD'] = None\n",
    "    category_map = {\n",
    "        'SINGLE FAMILY RESIDENTIAL': 'Residential - Single Family',\n",
    "        'SINGLE FAMILY RES TINY HOME': 'Residential - Single Family',\n",
    "        'SINGLE FAMILY MODULAR': 'Residential - Single Family',\n",
    "        'SINGLE FAMILY HISTORICAL PROPERTY': 'Residential - Single Family',\n",
    "        'DUPLEX-TRIPLEX': 'Residential - Multi Family',\n",
    "        'TOWNHOUSE': 'Residential - Multi Family',\n",
    "        'APARTMENT-GARDEN <=3 STORIES': 'Residential - Multi Family',\n",
    "        'RESIDENTIAL CONDOMINIUM < 7 STORIES': 'Residential - Multi Family',\n",
    "        'MID RISE APARTMENT 4-6 STORIES': 'Residential - Multi Family',\n",
    "        'RESIDENTIAL CONDO HI-RISE > 6 STORIES': 'Residential - Multi Family',\n",
    "        'APARTMENT-HIRISE >=7 STORIES': 'Residential - Multi Family',\n",
    "        'APARTMENT-TOWNHOUSE': 'Residential - Multi Family',\n",
    "        'PATIO HOME': 'Residential - Single Family',\n",
    "        'GROUP HOME': 'Residential - Multi Family',\n",
    "        'DORMITORY': 'Residential - Multi Family',\n",
    "        'OFFICE/RETAIL/MULTI-FAMILY': 'Mixed Use / Common Area',\n",
    "        'CLUB HOUSE HOA NO VALUE': 'Mixed Use / Common Area',\n",
    "        'NO VALUE IMPROVMENT': 'Mixed Use / Common Area',\n",
    "        'CLUB-LODGE': 'Mixed Use / Common Area',\n",
    "        'RECREATIONAL CENTER': 'Mixed Use / Common Area',\n",
    "        'RETAIL': 'Commercial / Retail',\n",
    "        'RETAIL CONDO': 'Commercial / Retail',\n",
    "        'RETAIL/CONVENIENCE STORE': 'Commercial / Retail',\n",
    "        'FURNITURE SHOWROOM': 'Commercial / Retail',\n",
    "        'DEALERSHIP SHOWROOM': 'Commercial / Retail',\n",
    "        'CONVENIENCE STORE': 'Commercial / Retail',\n",
    "        'DISCOUNT/DEPARTMENT STORE': 'Commercial / Retail',\n",
    "        'DEPARTMENT STORE': 'Commercial / Retail',\n",
    "        'MINI MART CONVENIENCE STORE': 'Commercial / Retail',\n",
    "        'SUPERMARKET': 'Commercial / Retail',\n",
    "        'STRIP SHOPPING CENTER': 'Commercial / Retail',\n",
    "        'SHOPPING MALL': 'Commercial / Retail',\n",
    "        'FAST FOOD': 'Commercial / Retail',\n",
    "        'FAST FOOD/CONVENIENCE STORE': 'Commercial / Retail',\n",
    "        'RESTAURANT': 'Commercial / Retail',\n",
    "        'NIGHTCLUB/LOUNGE': 'Commercial / Retail',\n",
    "        'DAYCARE': 'Commercial / Retail',\n",
    "        'BANK': 'Commercial / Retail',\n",
    "        'THEATRE': 'Commercial / Retail',\n",
    "        'FITNESS CENTER/HEALTH CLUB': 'Commercial / Retail',\n",
    "        'ANIMAL DAY SPA': 'Commercial / Retail',\n",
    "        'CAR WASH FULL-SERVICE TYPE': 'Commercial / Retail',\n",
    "        'CAR WASH SELF-SERVICE TYPE': 'Commercial / Retail',\n",
    "        'CAR WASH DRIVE-THRU TYPE': 'Commercial / Retail',\n",
    "        'MINI SPECIALTY AUTOMOTIVE': 'Commercial / Retail',\n",
    "        'AUTO SERVICE CENTER': 'Commercial / Retail',\n",
    "        'SERVICE GARAGE': 'Commercial / Retail',\n",
    "        'OFFICE A CLASS': 'Office',\n",
    "        'OFFICE B CLASS': 'Office',\n",
    "        'OFFICE C CLASS': 'Office',\n",
    "        'OFFICE HI-RISE A > 4 STORIES': 'Office',\n",
    "        'OFFICE HI-RISE B > 4 STORIES': 'Office',\n",
    "        'OFFICE MODULAR': 'Office',\n",
    "        'OFFICE CONDOMINIUM': 'Office',\n",
    "        'SFR TO OFFICE': 'Office',\n",
    "        'OFFICE & RETAIL': 'Office',\n",
    "        'LAB- RESEARCH': 'Office',\n",
    "        'MEDICAL/DENTAL': 'Medical',\n",
    "        'MEDICAL CONDOMINIUM': 'Medical',\n",
    "        'HOSPITAL-PRIVATE': 'Medical',\n",
    "        'HOSPITAL-PUBLIC': 'Medical',\n",
    "        'NURSING HOME/CONVALESCENT HOSPITAL': 'Medical',\n",
    "        'URGENT CARE FACILITY': 'Medical',\n",
    "        'VETERINARIAN OFFICE': 'Medical',\n",
    "        'ASSISTED LIVING': 'Medical',\n",
    "        'HOME FOR THE ELDERLY': 'Medical',\n",
    "        'HOTEL FULL SERVICE': 'Hospitality',\n",
    "        'HOTEL LIMITED SERVICE': 'Hospitality',\n",
    "        'HOTEL/EXTENDED STAY': 'Hospitality',\n",
    "        'MOTEL': 'Hospitality',\n",
    "        'MOTEL/HOTEL LODGING < 7 STORIES': 'Hospitality',\n",
    "        'HOTEL LODGING HI-RISE > 6 STORIES': 'Hospitality',\n",
    "        'BED & BREAKFEST': 'Hospitality',\n",
    "        'MICRO BREWERY/WINERY': 'Hospitality',\n",
    "        'WAREHOUSE': 'Industrial / Warehousing',\n",
    "        'MEGA WAREHOUSE': 'Industrial / Warehousing',\n",
    "        'MINI WAREHOUSE': 'Industrial / Warehousing',\n",
    "        'MINI WAREHOUSE CLIMATE CONTROL': 'Industrial / Warehousing',\n",
    "        'WAREHOUSE CONDOMINIUM': 'Industrial / Warehousing',\n",
    "        'WAREHOUSE DISTRIBUTION': 'Industrial / Warehousing',\n",
    "        'WAREHOUSE DISCOUNT STORE': 'Industrial / Warehousing',\n",
    "        'TRANSIT/TRUCK WAREHOUSE': 'Industrial / Warehousing',\n",
    "        'INDUSTRIAL': 'Industrial / Warehousing',\n",
    "        'INDUSTRIAL FLEX': 'Industrial / Warehousing',\n",
    "        'INDUSTRIAL RESEARCH & DEVELOPMENT': 'Industrial / Warehousing',\n",
    "        'LIGHT MANUFACTURING': 'Industrial / Warehousing',\n",
    "        'LIGHT MANUFACTURING > 75,000 SF': 'Industrial / Warehousing',\n",
    "        'HEAVY MANUFACTURING': 'Industrial / Warehousing',\n",
    "        'PREFAB WAREHOUSE': 'Industrial / Warehousing',\n",
    "        'BOTTLING PLANT': 'Industrial / Warehousing',\n",
    "        'PACKING PLANT': 'Industrial / Warehousing',\n",
    "        'COMPUTER DATA CENTER': 'Industrial / Warehousing',\n",
    "        'COLD STORAGE': 'Industrial / Warehousing',\n",
    "        'LUMBER YARD': 'Industrial / Warehousing',\n",
    "        'CHURCH': 'Religious / Church',\n",
    "        'SCHOOL-PUBLIC': 'Other / Infrastructure / Utilities',\n",
    "        'SCHOOL-PRIVATE': 'Other / Infrastructure / Utilities',\n",
    "        'COLLEGE/UNIVERSITY': 'Other / Infrastructure / Utilities',\n",
    "        'LIBRARY': 'Other / Infrastructure / Utilities',\n",
    "        'MUNICIPAL': 'Other / Infrastructure / Utilities',\n",
    "        'COUNTY': 'Other / Infrastructure / Utilities',\n",
    "        'STATE': 'Other / Infrastructure / Utilities',\n",
    "        'FEDERAL': 'Other / Infrastructure / Utilities',\n",
    "        'UTILITY/MECHANICAL EQUIPMENT BUILDING': 'Other / Infrastructure / Utilities',\n",
    "        'CELLULAR EQUIPMENT SHED - STATE CERTIFIED': 'Other / Infrastructure / Utilities',\n",
    "        'Passenger Terminal': 'Other / Infrastructure / Utilities',\n",
    "        'CONVENTION CENTER': 'Other / Infrastructure / Utilities',\n",
    "        'PARKING GARAGE': 'Other / Infrastructure / Utilities',\n",
    "        'UNDERGROUND PARKING': 'Other / Infrastructure / Utilities',\n",
    "        'STABLE': 'Other / Infrastructure / Utilities',\n",
    "        'COUNTRY CLUB': 'Other / Infrastructure / Utilities',\n",
    "        'STADIUM': 'Other / Infrastructure / Utilities',\n",
    "        'ARENA': 'Other / Infrastructure / Utilities',\n",
    "        'SPORTS ENTERTAINMENT CENTER': 'Other / Infrastructure / Utilities',\n",
    "        'MANUFACTURED HOME-DOUBLEWIDE': 'Residential - Single Family',\n",
    "        'MANUFACTURED HOME-SINGLEWIDE': 'Residential - Single Family',\n",
    "        'FUNERAL HOME/MORTUARY': 'Medical',\n",
    "        'DRUG STORE': 'Commercial / Retail',\n",
    "        'COMMERCIAL CONDOMINIUM': 'Commercial / Retail',\n",
    "        'MOTOR SPORTS GARAGE': 'Commercial / Retail',\n",
    "        'BOWLING ALLEY/SKATING RINK': 'Commercial / Retail',\n",
    "        'CONTINUING CARE': 'Medical',\n",
    "    } # Remapping the building types\n",
    "    tax_parcel_cama_dataset['bldgtype_grouped'] = tax_parcel_cama_dataset['bldgtype'].map(category_map)\n",
    "    \n",
    "    for idx, row in tqdm(ABT.iterrows(), total=len(ABT)):\n",
    "        poly = row.geometry\n",
    "        land_in_sub = tax_parcel_cama_dataset[tax_parcel_cama_dataset.geometry.intersects(poly)].copy()\n",
    "        \n",
    "        # Clip land use polygons to the subdivision\n",
    "        land_in_sub['geometry'] = land_in_sub.geometry.intersection(poly)\n",
    "        land_in_sub = land_in_sub[~land_in_sub.is_empty]\n",
    "        \n",
    "        if land_in_sub.empty:\n",
    "            ABT.at[idx, 'SHD'] = None\n",
    "            continue\n",
    "    \n",
    "        # Calculate area\n",
    "        land_in_sub['area'] = land_in_sub.geometry.area\n",
    "    \n",
    "        # Group by land use type and sum area\n",
    "        grouped = land_in_sub.groupby('bldgtype')['area'].sum()  #'bldgtype' column\n",
    "        total_area = grouped.sum()\n",
    "        proportions = grouped / total_area\n",
    "    \n",
    "        # Shannon entropy\n",
    "        S = len(proportions)\n",
    "        if S > 1:\n",
    "            shd = -np.sum(proportions * np.log(proportions)) / np.log(S)\n",
    "        else:\n",
    "            shd = 0  # Only one land use ‚Üí no diversity\n",
    "    \n",
    "        ABT.at[idx, 'SHD'] = shd\n",
    "        ABT['SHD'] = pd.to_numeric(ABT['SHD'], errors='coerce').round(2)\n",
    "        \n",
    "else:\n",
    "    print(\"‚úÖ SHD Index already exist).\")\n"
   ],
   "id": "1cb5220830e6aba3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6634/6634 [03:02<00:00, 36.42it/s]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __7. Measuring the 7th indicator (RJD) and the 8th indicator (AND)__",
   "id": "ba6cca2896c0f9e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:34:32.742974Z",
     "start_time": "2026-01-26T22:34:32.689754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network_measures = ['int_den0_1', 'nd_deg02_1', 'int_den0_2', 'nd_deg05_y', 'int_den0_3', 'nd_deg07_1', 'int_den1_1', 'nd_deg10_y']\n",
    "if not all(col in ABT.columns for col in network_measures):\n",
    "    ABT = pd.merge(ABT,street_network_df[['subd_id'] + network_measures],on=\"subd_id\", how=\"left\")\n",
    "else:\n",
    "    print(\"‚úÖ RJD and AND Index already exist.\")"
   ],
   "id": "6d3caa7a1f00309d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __9. Measuring the 9th indicator (APT) and the 10th indicator (AUO)__",
   "id": "45eb8531e347301b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:50:30.820008Z",
     "start_time": "2026-01-26T22:50:30.809708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accessibility_measures = [\"groceries_ws\", \"transit_ws\"]\n",
    "if not all(col in ABT.columns for col in accessibility_measures):\n",
    "    ABT = pd.merge(ABT,accessibility_ws[['subd_id'] + accessibility_measures],on=\"subd_id\", how=\"left\")\n",
    "else:\n",
    "    print(\"‚úÖ APT and AUO Index already exist.\")"
   ],
   "id": "880ada268035e911",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# accessibility_measures = [\"groceries_ws\", \"transit_ws\"]\n",
    "# missing_code = 999  # value representing missing\n",
    "# if not all(col in ABT.columns for col in accessibility_measures):\n",
    "#     ABT = pd.merge(ABT,accessibility_ws[['subd_id'] + accessibility_measures], on=\"subd_id\", how=\"left\")\n",
    "# else:\n",
    "#     print(\"‚úÖ APT and AUO Index already exist.\")\n",
    "# \n",
    "# \n",
    "# # Handling missing values\n",
    "# \n",
    "# # ----------------------------------------------\n",
    "# # STEP 1 ‚Äî Ensure required columns exist\n",
    "# # ----------------------------------------------\n",
    "# print(\"üîç Checking for required accessibility columns in ABT...\")\n",
    "# \n",
    "# missing_cols = [col for col in accessibility_measures if col not in ABT.columns]\n",
    "# \n",
    "# if missing_cols:\n",
    "#     print(f\"‚ö†Ô∏è Missing columns detected in ABT: {missing_cols}\")\n",
    "#     print(\"‚û°Ô∏è Attempting to merge them from accessibility_ws...\")\n",
    "# \n",
    "#     ABT = pd.merge(\n",
    "#         ABT,\n",
    "#         accessibility_ws[[\"subd_id\"] + missing_cols],\n",
    "#         on=\"subd_id\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "# \n",
    "#     print(f\"‚úî Successfully merged missing columns: {missing_cols}\")\n",
    "# else:\n",
    "#     print(\"‚úÖ All accessibility columns already exist in ABT.\")\n",
    "# \n",
    "# \n",
    "# # ----------------------------------------------\n",
    "# # STEP 2 ‚Äî Compute centroid coordinates\n",
    "# # ----------------------------------------------\n",
    "# print(\"\\nüìå Extracting centroid coordinates from ABT geometry...\")\n",
    "# \n",
    "# if ABT.geometry.is_empty.any():\n",
    "#     print(\"‚ö†Ô∏è Warning: Some geometries are empty. NN matching may fail for those rows.\")\n",
    "# \n",
    "# # Compute centroids safely\n",
    "# ABT[\"cx\"] = ABT.geometry.centroid.x\n",
    "# ABT[\"cy\"] = ABT.geometry.centroid.y\n",
    "# \n",
    "# print(\"‚úî Centroid coordinates computed: using columns 'cx' and 'cy'.\")\n",
    "# \n",
    "# \n",
    "# # ----------------------------------------------\n",
    "# # STEP 3 ‚Äî Spatial nearest-neighbor imputation\n",
    "# # ----------------------------------------------\n",
    "# print(\"\\nüîç Beginning spatial nearest-neighbor imputation for 999-coded missing values...\\n\")\n",
    "# \n",
    "# df = ABT.copy()  # Safe working copy\n",
    "# \n",
    "# for col in accessibility_measures:\n",
    "#     print(f\"--- Processing column: '{col}' ---\")\n",
    "# \n",
    "#     if col not in df.columns:\n",
    "#         print(f\"‚ùå Column '{col}' not found even after merge. Skipping.\")\n",
    "#         continue\n",
    "# \n",
    "#     # Identify missing vs. valid rows\n",
    "#     mask_missing = df[col] == missing_code\n",
    "#     mask_valid = ~mask_missing\n",
    "# \n",
    "#     num_missing = mask_missing.sum()\n",
    "#     num_valid = mask_valid.sum()\n",
    "# \n",
    "#     print(f\"‚Ä¢ Valid rows: {num_valid}\")\n",
    "#     print(f\"‚Ä¢ Missing (999) rows: {num_missing}\")\n",
    "# \n",
    "#     if num_missing == 0:\n",
    "#         print(f\"‚úî No 999-coded values in '{col}'. Moving on.\\n\")\n",
    "#         continue\n",
    "# \n",
    "#     # Coordinates\n",
    "#     coords_valid = df.loc[mask_valid, [\"cx\", \"cy\"]].values\n",
    "#     coords_missing = df.loc[mask_missing, [\"cx\", \"cy\"]].values\n",
    "# \n",
    "#     print(\"  ‚Üí Fitting NearestNeighbors model on valid spatial points...\")\n",
    "#     nn = NearestNeighbors(n_neighbors=1)\n",
    "#     nn.fit(coords_valid)\n",
    "# \n",
    "#     print(\"  ‚Üí Finding nearest valid neighbors...\")\n",
    "#     distances, indices = nn.kneighbors(coords_missing)\n",
    "# \n",
    "#     # Extract nearest values\n",
    "#     nearest_values = df.loc[mask_valid, col].iloc[indices.flatten()].values\n",
    "# \n",
    "#     # Impute\n",
    "#     df.loc[mask_missing, col] = nearest_values\n",
    "# \n",
    "#     print(f\"‚úî Imputed {num_missing} missing (999) values in '{col}' using nearest centroid.\\n\")\n",
    "# \n",
    "# \n",
    "# # ----------------------------------------------\n",
    "# # STEP 4 ‚Äî Finalize output\n",
    "# # ----------------------------------------------\n",
    "# ABT = df.drop(columns=[\"cx\", \"cy\"])  # Remove helper columns\n",
    "# print(\"üèÅ Spatial NN imputation complete. ABT updated successfully.\")"
   ],
   "id": "17d15c4c8255c56d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:50:13.881106Z",
     "start_time": "2026-01-26T22:50:13.870720Z"
    }
   },
   "cell_type": "code",
   "source": "ABT = ABT.drop(['groceries_ws_x', 'transit_ws_x'], axis=1)",
   "id": "a19d2fe3b9355f88",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:42:45.464210Z",
     "start_time": "2026-01-26T22:42:45.449879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ABT = ABT.rename(columns={\n",
    "    'int_den0_1': 'int_den025',\n",
    "    'nd_deg02_1': 'nd_deg025',\n",
    "    'int_den0_2': 'int_den05',\n",
    "    'nd_deg05_y': 'nd_deg05',\n",
    "    'int_den0_3': 'int_den075',\n",
    "    'nd_deg07_1': 'nd_deg075',\n",
    "    'int_den1_1': 'int_den1',\n",
    "    'nd_deg10_y': 'nd_deg1'\n",
    "})"
   ],
   "id": "b40d13df3a912e53",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:50:15.521707Z",
     "start_time": "2026-01-26T22:50:15.504632Z"
    }
   },
   "cell_type": "code",
   "source": "ABT.columns",
   "id": "c02f73952531e614",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subd_id', 'issue_date', 'year', 'HAC_dist', 'BAD', 'FAR', 'SHD',\n",
       "       'area_acre', 'AI', 'PROX', 'ENN_MN', 'ED', 'SHAPE_MN', 'FRAC_MN',\n",
       "       'ENN_inv', 'ED_inv', 'SHAPE_inv', 'FRAC_inv', 'AI_norm', 'PROX_norm',\n",
       "       'ENN_inv_norm', 'ED_inv_norm', 'SHAPE_inv_norm', 'FRAC_inv_norm',\n",
       "       'COMPACTNESS_SUM', 'int_den025', 'nd_deg025', 'int_den05', 'nd_deg05',\n",
       "       'int_den075', 'nd_deg075', 'int_den1', 'nd_deg1', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:50:43.122928Z",
     "start_time": "2026-01-26T22:50:42.264657Z"
    }
   },
   "cell_type": "code",
   "source": "ABT.to_file(\"../../../Data/Final_dataset/ABT/ABT.gpkg\", layer=\"subdivisions\")",
   "id": "2daba2973eca72a3",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# __11. Measuring the 11th indicator (Green Coverage)__\n",
   "id": "847086a664daec4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "d3fa2d4aed90327f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
