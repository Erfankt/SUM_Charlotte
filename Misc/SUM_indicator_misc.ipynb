{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from rasterio import features\n",
    "from folium.plugins import Search\n",
    "import rasterio, folium, fiona, warnings\n",
    "from shapely.geometry import Point, Polygon, mapping\n",
    "from shapely.ops import nearest_points, unary_union   # correct\n",
    "from shapely.strtree import STRtree\n",
    "from scipy.spatial import distance_matrix"
   ],
   "id": "a9e9bb2188bd750b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if \"IMI_adj\" not in ABT.columns:\n",
    "    \n",
    "    ABT[\"IMI_adj\"] = None\n",
    "    \n",
    "    # ======================================================\n",
    "    # 1. INITIALIZE THE IMI ENGINE\n",
    "    # ======================================================\n",
    "    imi_engine = IMICompactness(\n",
    "        buildings=building_footprint_dataset,\n",
    "        waterbody=waterbody,\n",
    "        crs=ABT.crs\n",
    "    )\n",
    "    \n",
    "    # ======================================================\n",
    "    # 2. HELPER FUNCTION TO COMPUTE IMI FOR ONE SUBDIVISION\n",
    "    # ======================================================\n",
    "    def compute_imi_adj_for_row(sub_geom):\n",
    "        \"\"\"\n",
    "        Computes IMI_adjusted for one subdivision polygon.\n",
    "    \n",
    "        Returns:\n",
    "            float or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            imi_value, _ = imi_engine.compute_for_subdivision(\n",
    "                subdivision=sub_geom,\n",
    "                resolution=5,     # 5m cell size = good speed/accuracy balance\n",
    "                frac=1.0,         # use 100% of buildings\n",
    "                adjusted=True     # Compute IMI_adjusted !!!\n",
    "            )\n",
    "            return imi_value\n",
    "        except Exception as e:\n",
    "            print(\"Error computing IMI for subdivision:\", e)\n",
    "            return None\n",
    "\n",
    "    \n",
    "    # ======================================================\n",
    "    # 3. APPLY TO ALL SUBDIVISIONS WITH PROGRESS BAR\n",
    "    # ======================================================\n",
    "    tqdm.pandas()\n",
    "    print(\"Computing IMI_adjusted for all subdivisions...\")\n",
    "    ABT[\"IMI_adj\"] = ABT.geometry.progress_apply(compute_imi_adj_for_row)\n",
    "    print(\"DONE.\")\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… IMI Index already exist).\")"
   ],
   "id": "65009b4493ab20c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------------------\n",
    "# 0. SELECT METRICS\n",
    "# ----------------------------------------------------------\n",
    "metrics_used = [\n",
    "    \"AI\", \n",
    "    \"ENN_MN\",\n",
    "    \"PROX\",\n",
    "    \"ED\",\n",
    "    \"SHAPE_MN\",\n",
    "    \"FRAC_MN\"\n",
    "]\n",
    "\n",
    "df = ABT.copy()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. DIRECTIONAL ALIGNMENT\n",
    "# ----------------------------------------------------------\n",
    "df[\"ENN_inv\"] = 1 / (df[\"ENN_MN\"] + 1e-6)\n",
    "df[\"ED_inv\"]  = 1 / (df[\"ED\"] + 1e-6)\n",
    "\n",
    "metrics_final = [\n",
    "    \"AI\", \n",
    "    \"PROX\",\n",
    "    \"SHAPE_MN\",\n",
    "    \"FRAC_MN\",\n",
    "    \"ENN_inv\",\n",
    "    \"ED_inv\"\n",
    "]\n",
    "\n",
    "# Extract matrix\n",
    "X = df[metrics_final].copy()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. HANDLE MISSING VALUES\n",
    "# ----------------------------------------------------------\n",
    "# Drop rows where ALL metrics are missing\n",
    "X = X.dropna(how=\"all\")\n",
    "\n",
    "# Impute remaining missing values with column means\n",
    "X_imputed = X.fillna(X.mean())\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Z-SCORE NORMALIZATION\n",
    "# ----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "X_norm_df = pd.DataFrame(\n",
    "    X_scaled, \n",
    "    index=X_imputed.index, \n",
    "    columns=[m + \"_z\" for m in metrics_final]\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. PCA (PC1 as compactness index)\n",
    "# ----------------------------------------------------------\n",
    "pca = PCA(n_components=1)\n",
    "PC1_scores = pca.fit_transform(X_scaled).flatten()\n",
    "\n",
    "df.loc[X_norm_df.index, \"COMPACTNESS_PCA\"] = PC1_scores\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5. PCA LOADINGS\n",
    "# ----------------------------------------------------------\n",
    "loadings = pd.Series(\n",
    "    pca.components_[0],\n",
    "    index=metrics_final,\n",
    "    name=\"PC1_Loading\"\n",
    ")\n",
    "\n",
    "abs_weights = (loadings.abs() / loadings.abs().sum())\n",
    "abs_weights.name = \"PCA_Weight\"\n",
    "\n",
    "print(\"\\n===== PCA LOADINGS =====\")\n",
    "print(loadings)\n",
    "\n",
    "print(\"\\n===== PCA WEIGHTS (Normalized ABS values) =====\")\n",
    "print(abs_weights)\n",
    "\n",
    "print(\"\\nExplained variance (PC1):\", pca.explained_variance_ratio_[0])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6. STORE RESULTS\n",
    "# ----------------------------------------------------------\n",
    "ABT[\"COMPACTNESS_PCA\"] = df[\"COMPACTNESS_PCA\"]\n",
    "\n",
    "print(\"\\nðŸŽ¯ PCA Compactness Index successfully created (NaN-safe)!\")\n"
   ],
   "id": "d92f28fe547abdfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "461adaa383b7cb7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# if 'FAR' not in ABT.columns:\n",
    "#     \n",
    "#     ABT[\"FAR\"] = None\n",
    "#     \n",
    "#     steps = [\n",
    "#         \"Overlaying parcels and subdivisions...\",\n",
    "#         \"Computing overlap ratios...\",\n",
    "#         \"Filtering parcels with â‰¥80% overlap...\",\n",
    "#         \"Summing total floor area per subdivision...\",\n",
    "#         \"Merging and calculating FAR...\",\n",
    "#         \"Cleaning up columns...\"\n",
    "#     ]\n",
    "#     \n",
    "#     tax_parcel_cama_dataset[\"Shape_Area\"] = tax_parcel_cama_dataset.geometry.area\n",
    "#     for step in tqdm(steps, desc=\"Processing FAR computation\", ncols=100):\n",
    "#         if step == steps[0]:\n",
    "#             # --- 1. Overlay parcels and subdivisions to find intersections ---\n",
    "#             intersections = gpd.overlay(\n",
    "#                 tax_parcel_cama_dataset[[\"parcelid\", \"totalarea\",\"Shape_Area\", \"geometry\"]],\n",
    "#                 ABT[[\"subd_id\", \"geometry\"]],\n",
    "#                 how=\"intersection\",\n",
    "#                 keep_geom_type=True\n",
    "#             )\n",
    "# \n",
    "#         elif step == steps[1]:\n",
    "#             # --- 2. Compute overlap ratio for each intersected parcel ---\n",
    "#             intersections[\"intersect_area\"] = intersections.geometry.area\n",
    "#             intersections[\"overlap_ratio\"] = intersections[\"intersect_area\"] / intersections[\"Shape_Area\"]\n",
    "# \n",
    "#         elif step == steps[2]:\n",
    "#             # --- 3. Keep only parcels with â‰¥ 80% of their area inside subdivision ---\n",
    "#             intersections = intersections[intersections[\"overlap_ratio\"] >= 0.4]\n",
    "# \n",
    "#         elif step == steps[3]:\n",
    "#             # --- 4. Compute true subdivision-level FAR (Î£ total floor area Ã· ABT area) ---\n",
    "#             far_sum = intersections.groupby(\"subd_id\")[\"totalarea\"].sum().reset_index(name=\"total_floor_area\")\n",
    "# \n",
    "#         elif step == steps[4]:\n",
    "#             # Add subdivision area and merge\n",
    "#             ABT[\"abt_area\"] = ABT.geometry.area\n",
    "#             ABT = ABT.merge(far_sum, on=\"subd_id\", how=\"left\")\n",
    "#             ABT[\"FAR\"] = ABT[\"total_floor_area\"] / ABT[\"abt_area\"]\n",
    "#             ABT[\"FAR\"] = ABT[\"FAR\"].fillna(0).round(4)\n",
    "# \n",
    "#         elif step == steps[5]:\n",
    "#             # --- 6. Drop temporary columns if desired ---\n",
    "#             ABT = ABT.drop(columns=[\"abt_area\"], errors=\"ignore\")\n",
    "#             \n",
    "# else:\n",
    "#     print(\"âœ… FAR Index already exist).\")"
   ],
   "id": "6f043a47aa6034d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f74b6910f7bacc3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# if not all(col in ABT.columns for col in ['APT', 'APT_station', 'AUO','AUO_Grocery']):\n",
    "#     ABT = pd.merge(ABT,accessibility_dataset, on=\"subd_id\", how=\"left\")\n",
    "# if not all(col in ABT.columns for col in ['walk_time_min_transit', 'walk_time_min_amenity']):\n",
    "#     ABT = pd.merge(ABT,accessibility_dataset_osm, on=\"subd_id\", how=\"left\")\n",
    "# if \"avg_time_transit\" not in ABT.columns:\n",
    "#     ABT[\"avg_time_transit\"] = np.nan\n",
    "#     ABT[\"avg_time_transit\"] = (ABT[\"walk_time_min_osm\"] + ABT[\"APT\"])/ 2\n",
    "# else:\n",
    "#     print(\"âœ… All accessibility columns already exist.\")\n",
    "#     \n",
    "# #Fill missing walk_time_min_osm values using APT\n",
    "# # ABT['walk_time_min_osm'] = ABT['walk_time_min_osm'].fillna(ABT['APT'])"
   ],
   "id": "a3daf4f5bf804457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -----------------------------------------\n",
    "# CUSTOM VARIMAX (works for any k â‰¥ 1)\n",
    "# -----------------------------------------\n",
    "def varimax(Phi, gamma=1.0, q=20, tol=1e-6):\n",
    "    Phi = np.asarray(Phi)\n",
    "    p, k = Phi.shape\n",
    "\n",
    "    # 1-factor: rotation is identity\n",
    "    if k == 1:\n",
    "        return Phi\n",
    "\n",
    "    R = np.eye(k)\n",
    "    d = 0\n",
    "\n",
    "    for i in range(q):\n",
    "        d_old = d\n",
    "        Lambda = Phi @ R\n",
    "        u, s, vh = np.linalg.svd(\n",
    "            Phi.T @ (Lambda**3 - (gamma/p) * Lambda @ np.diag(np.diag(Lambda.T @ Lambda)))\n",
    "        )\n",
    "        R = u @ vh\n",
    "        d = np.sum(s)\n",
    "        if d_old != 0 and d / d_old < 1 + tol:\n",
    "            break\n",
    "\n",
    "    return Phi @ R\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 0. PREP â€” Fix directional metrics\n",
    "# -----------------------------------------\n",
    "df = ABT.copy()\n",
    "df[\"ENN_inv\"] = 1 / (df[\"ENN_MN\"] + 1e-6)\n",
    "df[\"ED_inv\"]  = 1 / (df[\"ED\"] + 1e-6)\n",
    "\n",
    "metrics_final = ['AI', 'PROX', 'ENN_inv', 'ED_inv', 'SHAPE_MN', 'FRAC_MN']\n",
    "\n",
    "X = df[metrics_final].copy().dropna(how=\"all\")\n",
    "X_imputed = X.fillna(X.mean())\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# MAIN FUNCTION: RUN PCA + ROTATION FOR ANY k COMPONENTS\n",
    "# ========================================================\n",
    "def run_pca_rotated(X_scaled, metrics, n_components):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"       ðŸ”· PCA WITH {n_components} ROTATED COMPONENTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # -------------------------\n",
    "    # Run PCA\n",
    "    # -------------------------\n",
    "    pca = PCA(n_components=n_components)\n",
    "    scores = pca.fit_transform(X_scaled)\n",
    "    loadings = pca.components_.T\n",
    "    eigenvalues = pca.explained_variance_\n",
    "\n",
    "    print(\"\\n=== EIGENVALUES ===\")\n",
    "    for i, eig in enumerate(eigenvalues):\n",
    "        print(f\"PC{i+1}: {eig:.4f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Rotate loadings\n",
    "    # -------------------------\n",
    "    rotated = varimax(loadings)\n",
    "\n",
    "    print(\"\\n=== ROTATED LOADINGS ===\")\n",
    "    rotated_df = pd.DataFrame(rotated, index=metrics, columns=[f\"RC{i+1}\" for i in range(n_components)])\n",
    "    print(rotated_df)\n",
    "\n",
    "    # -------------------------\n",
    "    # Compute normalized weights per component\n",
    "    # -------------------------\n",
    "    print(\"\\n=== NORMALIZED WEIGHTS PER COMPONENT ===\")\n",
    "    for comp in range(n_components):\n",
    "        abs_load = np.abs(rotated[:, comp])\n",
    "        weights = abs_load / abs_load.sum()\n",
    "        print(f\"\\n--- Component RC{comp+1} Weights ---\")\n",
    "        print(pd.Series(weights, index=metrics))\n",
    "\n",
    "    return {\n",
    "        \"eigenvalues\": eigenvalues,\n",
    "        \"rotated_loadings\": rotated_df,\n",
    "        \"scores\": scores,\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# RUN FOR 2, 3, AND 4 COMPONENTS\n",
    "# ========================================================\n",
    "results_2 = run_pca_rotated(X_scaled, metrics_final, 2)\n",
    "results_3 = run_pca_rotated(X_scaled, metrics_final, 3)\n",
    "results_4 = run_pca_rotated(X_scaled, metrics_final, 4)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Completed all PCA rotated solutions (2, 3, 4 components).\")"
   ],
   "id": "7e410c4644fde570"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
